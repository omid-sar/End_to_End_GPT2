{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXJOx/QHHpYzmGk7eNAKu5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omid-sar/End_to_End_GPT2/blob/main/src/GPT2/research/tokenize_and_shard_wikitext_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --q tiktoken\n",
        "!pip install --q datasets\n",
        "!pip install --q tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxCnL-3Hy5Oe",
        "outputId": "e4e9d380-c8c1-4953-de05-950ae29af35c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "FineWeb-Edu dataset (for srs pretraining)\n",
        "https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu\n",
        "Downloads and tokenizes the data and saves data shards to disk.\n",
        "Run simply as:\n",
        "$ python fineweb.py\n",
        "Will save shards to the local directory \"edu_fineweb10B\".\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import multiprocessing as mp\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "from datasets import load_dataset # pip install datasets\n",
        "from tqdm import tqdm # pip install tqdm\n",
        "\n",
        "# ------------------------------------------\n",
        "local_dir = \"WIKI\"\n",
        "remote_name = \"wikitext-2-raw-v1\" #\"sample-10BT\"\n",
        "shard_size = int(1e6) # 100M tokens per shard, total of 100 shards\n",
        "\n",
        "# create the cache the local directory if it doesn't exist yet\n",
        "DATA_CACHE_DIR = os.path.join(os.getcwd(), local_dir)\n",
        "os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
        "\n",
        "# download the dataset\n",
        "fw = load_dataset(\"wikitext\", name=remote_name, split=\"train\")\n",
        "\n",
        "# init the tokenizer\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "eot = enc._special_tokens['<|endoftext|>'] # end of text token\n",
        "def tokenize(doc):\n",
        "    # tokenizes a single document and returns a numpy array of uint16 tokens\n",
        "    tokens = [eot] # the special <|endoftext|> token delimits all documents\n",
        "    tokens.extend(enc.encode_ordinary(doc[\"text\"]))\n",
        "    tokens_np = np.array(tokens)\n",
        "    assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token dictionary too large for uint16\"\n",
        "    tokens_np_uint16 = tokens_np.astype(np.uint16)\n",
        "    return tokens_np_uint16\n",
        "\n",
        "def write_datafile(filename, tokens_np):\n",
        "    np.save(filename, tokens_np)\n",
        "\n",
        "# tokenize all documents and write output shards, each of shard_size tokens (last shard has remainder)\n",
        "nprocs = max(1, os.cpu_count()//2)\n",
        "with mp.Pool(nprocs) as pool:\n",
        "    shard_index = 0\n",
        "    # preallocate buffer to hold current shard\n",
        "    all_tokens_np = np.empty((shard_size,), dtype=np.uint16)\n",
        "    token_count = 0\n",
        "    progress_bar = None\n",
        "    for tokens in pool.imap(tokenize, fw, chunksize=16):\n",
        "\n",
        "        # is there enough space in the current shard for the new tokens?\n",
        "        if token_count + len(tokens) < shard_size:\n",
        "            # simply append tokens to current shard\n",
        "            all_tokens_np[token_count:token_count+len(tokens)] = tokens\n",
        "            token_count += len(tokens)\n",
        "            # update progress bar\n",
        "            if progress_bar is None:\n",
        "                progress_bar = tqdm(total=shard_size, unit=\"tokens\", desc=f\"Shard {shard_index}\")\n",
        "            progress_bar.update(len(tokens))\n",
        "        else:\n",
        "            # write the current shard and start a new one\n",
        "            split = \"val\" if shard_index == 0 else \"train\"\n",
        "            filename = os.path.join(DATA_CACHE_DIR, f\"edufineweb_{split}_{shard_index:06d}\")\n",
        "            # split the document into whatever fits in this shard; the remainder goes to next one\n",
        "            remainder = shard_size - token_count\n",
        "            progress_bar.update(remainder)\n",
        "            all_tokens_np[token_count:token_count+remainder] = tokens[:remainder]\n",
        "            write_datafile(filename, all_tokens_np)\n",
        "            shard_index += 1\n",
        "            progress_bar = None\n",
        "            # populate the next shard with the leftovers of the current doc\n",
        "            all_tokens_np[0:len(tokens)-remainder] = tokens[remainder:]\n",
        "            token_count = len(tokens)-remainder\n",
        "\n",
        "    # write any remaining tokens as the last shard\n",
        "    if token_count != 0:\n",
        "        split = \"val\" if shard_index == 0 else \"train\"\n",
        "        filename = os.path.join(DATA_CACHE_DIR, f\"edufineweb_{split}_{shard_index:06d}\")\n",
        "        write_datafile(filename, all_tokens_np[:token_count])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXJB8OEi1VLw",
        "outputId": "8e9ad19a-6690-44be-e3b4-7ec9809662ac"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Shard 2:  43%|████▎     | 428539/1000000 [12:21<16:29, 577.59tokens/s]   \n",
            "\n",
            "Shard 0:   4%|▍         | 38317/1000000 [00:00<00:02, 382050.89tokens/s]\u001b[A\n",
            "Shard 0:   8%|▊         | 83989/1000000 [00:00<00:02, 421860.35tokens/s]\u001b[A\n",
            "Shard 0:  13%|█▎        | 134582/1000000 [00:00<00:01, 458584.32tokens/s]\u001b[A\n",
            "Shard 0:  18%|█▊        | 182558/1000000 [00:00<00:01, 464334.31tokens/s]\u001b[A\n",
            "Shard 0:  23%|██▎       | 230938/1000000 [00:00<00:01, 470313.82tokens/s]\u001b[A\n",
            "Shard 0:  28%|██▊       | 278018/1000000 [00:00<00:01, 438655.82tokens/s]\u001b[A\n",
            "Shard 0:  32%|███▏      | 322264/1000000 [00:00<00:01, 419593.31tokens/s]\u001b[A\n",
            "Shard 0:  36%|███▋      | 364578/1000000 [00:00<00:01, 399775.43tokens/s]\u001b[A\n",
            "Shard 0:  41%|████      | 410048/1000000 [00:00<00:01, 413185.12tokens/s]\u001b[A\n",
            "Shard 0:  46%|████▌     | 459936/1000000 [00:01<00:01, 435421.86tokens/s]\u001b[A\n",
            "Shard 0:  50%|█████     | 504076/1000000 [00:01<00:01, 434272.80tokens/s]\u001b[A\n",
            "Shard 0:  55%|█████▍    | 548019/1000000 [00:01<00:01, 434894.22tokens/s]\u001b[A\n",
            "Shard 0:  59%|█████▉    | 592915/1000000 [00:01<00:00, 438914.83tokens/s]\u001b[A\n",
            "Shard 0:  64%|██████▍   | 642002/1000000 [00:01<00:00, 453375.48tokens/s]\u001b[A\n",
            "Shard 0:  69%|██████▉   | 689305/1000000 [00:01<00:00, 457462.62tokens/s]\u001b[A\n",
            "Shard 0:  74%|███████▎  | 735183/1000000 [00:01<00:00, 443801.82tokens/s]\u001b[A\n",
            "Shard 0:  78%|███████▊  | 779856/1000000 [00:01<00:00, 417878.74tokens/s]\u001b[A\n",
            "Shard 0:  82%|████████▏ | 822081/1000000 [00:01<00:00, 416819.66tokens/s]\u001b[A\n",
            "Shard 0:  86%|████████▋ | 864055/1000000 [00:02<00:00, 371700.94tokens/s]\u001b[A\n",
            "Shard 0:  90%|█████████ | 902233/1000000 [00:02<00:00, 314756.53tokens/s]\u001b[A\n",
            "Shard 0:  94%|█████████▎| 935666/1000000 [00:02<00:00, 268123.20tokens/s]\u001b[A\n",
            "Shard 0:  96%|█████████▋| 964646/1000000 [00:02<00:00, 240428.83tokens/s]\u001b[A\n",
            "Shard 0: 100%|██████████| 1000000/1000000 [00:02<00:00, 344836.99tokens/s]\n",
            "Shard 1:  97%|█████████▋| 965518/1000000 [00:03<00:00, 464205.24tokens/s]\n",
            "Shard 2:   0%|          | 0/1000000 [00:00<?, ?tokens/s]\u001b[A\n",
            "Shard 1: 100%|█████████▉| 999713/1000000 [00:03<00:00, 255578.76tokens/s]\n",
            "\n",
            "Shard 2:  13%|█▎        | 126238/1000000 [00:00<00:01, 497981.65tokens/s]\u001b[A\n",
            "Shard 2:  18%|█▊        | 177488/1000000 [00:00<00:01, 465971.47tokens/s]\u001b[A\n",
            "Shard 2:  23%|██▎       | 227535/1000000 [00:00<00:01, 463461.34tokens/s]\u001b[A\n",
            "Shard 2:  28%|██▊       | 275530/1000000 [00:00<00:01, 463136.51tokens/s]\u001b[A\n",
            "Shard 2:  33%|███▎      | 326274/1000000 [00:00<00:01, 471264.82tokens/s]\u001b[A\n",
            "Shard 2:  37%|███▋      | 373699/1000000 [00:00<00:01, 461167.64tokens/s]\u001b[A\n",
            "Shard 2:  42%|████▏     | 420774/1000000 [00:00<00:01, 463173.32tokens/s]\u001b[A"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6jwRn2ifaY46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L4PzPj5kaZF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y_FSjmlPaZQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yXVczNXQ1s0E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
artifacts_root: artifacts

data_ingestion:
  root_dir: artifacts/data_ingestion
  dataset_name: "wikitext-2-raw-v1" #***"sample-10BT"
  dataset: "wikitext" #***"HuggingFaceFW/fineweb-edu"
  local_data_file: artifacts/data_ingestion/data

data_transformation:
  root_dir: artifacts/data_transformation
  dataset_name: "wikitext-2-raw-v1" #***"sample-10BT"
  dataset: "wikitext" #***"HuggingFaceFW/fineweb-edu"
  downloaded_files: artifacts/data_ingestion/data
  local_data_file: artifacts/data_transformation/data
  shard_size: 1000000 #*** 100000000 small shard size for toy "wikitext" dataset
  total_batch_size: 8192 #***524299 # 2^19 based on 124M GPT3 mdoel
  B: 4 # Micro batch size #***64
  T: 1024 # Sequence length

gpt_config:
  root_dir: artifacts/model_config
  verification_info_dir: artifacts/model_config/verification_info
  verification_summary_file: verification_model_summary.txt
  verification_weights_file: verification_initial_weights.pt
  block_size: 1024 # Sequence Length
  vocab_size: 50304
  n_layer: 12
  n_head: 12
  n_embd : 768 
  weight_decay: 0.1
  learning_rate: 0.0006
  


model_training:
  root_dir: artifacts/model_training
  model_folder: artifacts/model_training/gpt2_weights
  total_batch_size: 8192 #***524299 # 2^19 based on 124M GPT3 mdoel
  B: 4 # Micro batch size #***64
  T: 1024 # Sequence length
  max_lr: 0.0006
  min_lr: 0.00006 #max_lr * 0.1
  warmup_steps: 2
  max_steps: 4

model_evaluation:
  root_dir: artifacts/model_evaluation

model_inference:
root_dir: artifacts/model_inference


